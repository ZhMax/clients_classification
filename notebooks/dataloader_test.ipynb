{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_732/2781985143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as pa_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Any, Dict, List, Union, Tuple\n",
    "\n",
    "try:\n",
    "    from typing import Literal\n",
    "except ImportError:\n",
    "    from typing_extensions import Literal\n",
    "\n",
    "\n",
    "class IndexedDatasetFromFiles(Dataset):\n",
    "    \"\"\"Dataset for loading data vectors from `.npy` files.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        example_names: list,\n",
    "        features: Dict[str, np.array],\n",
    "        targets: Dict[str, np.array] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "            example_names: list\n",
    "                List contains names of examples from a dataset\n",
    "\n",
    "            features: Dict[str, float]\n",
    "                Dictionary contains for each example its name and \n",
    "                vector with features\n",
    "\n",
    "            targets: Dict[str, np.array] = None\n",
    "                Dictionary contains for each example its name and \n",
    "                true label\n",
    "        \"\"\"\n",
    "\n",
    "        super(IndexedDatasetFromFiles, self).__init__()\n",
    "\n",
    "        self._example_names = example_names\n",
    "        \n",
    "        self._features = features\n",
    "        self._targets = targets\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Get vectors with features and label by idx of `file_name` \n",
    "        in `all_data_files`\n",
    "        \"\"\"\n",
    "\n",
    "        example_name = self._example_names[idx]\n",
    "        x = self._features[example_name]\n",
    "        x = np.asarray(x)\n",
    "        x = torch.from_numpy(x).float()\n",
    "\n",
    "        if self._targets is not None:\n",
    "            target = self._targets[example_name]\n",
    "            y = np.asarray(target)\n",
    "            y = torch.from_numpy(y).int()\n",
    "            return x, y, idx\n",
    "\n",
    "        else:\n",
    "            y = None\n",
    "            return x, idx\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._example_names)\n",
    "\n",
    "    \n",
    "    def get_file_name(self, idx: int):\n",
    "        \"\"\"Get example by idx\"\"\"\n",
    "        example_name = self._example_names[idx]\n",
    "        return example_name\n",
    "\n",
    "\n",
    "def split_array_into_twoparts_by_inds(\n",
    "    ar: np.array,\n",
    "    random_state: int,\n",
    "    split_fraction: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Divide input array by two parts and return indices \n",
    "    for each part\n",
    "\n",
    "    Args:\n",
    "        ar: np.array\n",
    "            Input array\n",
    "\n",
    "        random_state: int\n",
    "            To provide reproducibility\n",
    "\n",
    "        split_fraction: float\n",
    "        Relation between sizes of the first part and the input array.\n",
    "        If it is equal 1.0, the first part size is equal to \n",
    "        the input array size.\n",
    "    \"\"\"\n",
    "    original_ids = np.array(range(len(ar)))\n",
    "    \n",
    "    inds_pt1, inds_pt2 = train_test_split(\n",
    "        original_ids, \n",
    "        train_size=split_fraction, \n",
    "        random_state=random_state, \n",
    "        shuffle=False)\n",
    "\n",
    "    return inds_pt1, inds_pt2\n",
    "\n",
    "\n",
    "def create_datasets(\n",
    "    features_path: str,\n",
    "    random_state: int,\n",
    "    features_dim: int,\n",
    "    mode: Literal['predict', 'fit', 'forgetting', 'second-split-forgetting'],\n",
    "    targets_path: str = None,\n",
    "    path_to_file_names_to_be_excluded: str = None,\n",
    "    split_fraction: float = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create datasets from files containing in the directory `data_filepath` \n",
    "\n",
    "    Args:\n",
    "        data_filepath: str\n",
    "            Path to a directory which contains all files of the dataset.\n",
    "\n",
    "        random_state: int\n",
    "            To provide reproducibility.\n",
    "\n",
    "        features_dim: int\n",
    "            Dimension (the number of components) of the feature vector\n",
    "\n",
    "        mode: str\n",
    "            It takes one of the values 'predict', 'fit', 'forgetting' or 'second-split-forgetting'. \n",
    "            Depending on the value of the argument, datasets will be created to train \n",
    "            the model, to get predictions or to find noisy examples by forgetting methods\n",
    "        \n",
    "        targets_path: str = None\n",
    "            Path to a directory which contains files with true labels.\n",
    "            It is supposed that the files containing features and true label \n",
    "            related to one example from the dataset have the same name. If it is `None`, \n",
    "            target variable will not be returned.\n",
    "\n",
    "        path_to_file_names_to_be_excluded: str\n",
    "            Path to a `.txt` file which contains names of files \n",
    "            to be excluded from the original dataset.\n",
    "\n",
    "        split_fraction: float\n",
    "        Relation between sizes of the first part and the input array.\n",
    "    \"\"\"\n",
    "\n",
    "    #Load pyarrow datasets\n",
    "    ds_features = pa_dataset.dataset(features_path)\n",
    "    ds_targets = pa_dataset.dataset(targets_path)\n",
    "\n",
    "    #get np arrays\n",
    "    if path_to_file_names_to_be_excluded is None:\n",
    "        example_names = np.array(\n",
    "            ds_features.scanner(\n",
    "                columns=['__index_level_0__']\n",
    "            ).to_table()\n",
    "        )[0]\n",
    "\n",
    "        features = np.array(\n",
    "            ds_features.scanner(\n",
    "                columns=[str(item) for item in range(0, features_dim)]\n",
    "            ).to_table()\n",
    "        )\n",
    "\n",
    "        targets = ds_targets.to_table()\n",
    "        labels = np.array(targets[0])\n",
    "        target_names = np.array(targets[1])\n",
    "\n",
    "    else:\n",
    "        file_name = path_to_file_names_to_be_excluded\n",
    "        excluded_names = np.loadtxt(file_name, delimiter=' ', dtype='str')\n",
    "\n",
    "        example_names = np.array(\n",
    "            ds_features.scanner(\n",
    "                columns=['__index_level_0__'],\n",
    "                filter=(~pa_dataset.field('__index_level_0__').isin(excluded_names))\n",
    "            ).to_table()\n",
    "        )[0]\n",
    "\n",
    "        features = np.array(\n",
    "            ds_features.scanner(\n",
    "                columns=[str(item) for item in range(0, features_dim)],\n",
    "                filter=(~pa_dataset.field('__index_level_0__').isin(excluded_names))\n",
    "            ).to_table()\n",
    "        )\n",
    "\n",
    "        targets = ds_targets.scanner(\n",
    "            filter=(~pa_dataset.field('__index_level_0__').isin(excluded_names))\n",
    "        ).to_table()\n",
    "        labels = np.array(targets[0])\n",
    "        target_names = np.array(targets[1])\n",
    "\n",
    "        print(f\"From the dataset {len(excluded_names)} files are excluded.\")\n",
    "\n",
    "    #create dictionaries with features and targets\n",
    "    features = features.T\n",
    "    features = {k: v for k, v in zip(example_names, features)}\n",
    "\n",
    "    targets = {k: int(v) for k, v in zip(target_names, labels)}\n",
    "\n",
    "    #create torch dataset\n",
    "    dataset = IndexedDatasetFromFiles(example_names, features, targets)\n",
    "\n",
    "\n",
    "    #create subsets\n",
    "    if mode == 'predict':\n",
    "        dataset_pt1 = dataset\n",
    "        dataset_pt2 = None\n",
    "\n",
    "        return dataset_pt1\n",
    "\n",
    "    elif mode == 'fit':\n",
    "        if split_fraction is None:\n",
    "            split_fraction = 1.0\n",
    "\n",
    "        if split_fraction < 1.0:\n",
    "\n",
    "            inds_pt1, inds_pt2 = split_array_into_twoparts_by_inds(\n",
    "                example_names, \n",
    "                random_state,\n",
    "                split_fraction\n",
    "            )\n",
    "\n",
    "            dataset_pt1 = Subset(dataset, inds_pt1)\n",
    "            dataset_pt2 = Subset(dataset, inds_pt2)\n",
    "        \n",
    "        else:\n",
    "            dataset_pt1 = dataset\n",
    "            dataset_pt2 = dataset\n",
    "\n",
    "        return dataset_pt1, dataset_pt2\n",
    "    \n",
    "    elif mode == 'forgetting':\n",
    "        dataset_pt1 = dataset\n",
    "        dataset_pt2 = None\n",
    "\n",
    "        return dataset_pt1\n",
    "    \n",
    "    elif mode == 'second-split-forgetting':\n",
    "        if split_fraction is None:\n",
    "            split_fraction = 0.5\n",
    "\n",
    "        inds_pt1, inds_pt2 = split_array_into_twoparts_by_inds(\n",
    "            example_names, \n",
    "            random_state,\n",
    "            split_fraction\n",
    "        )\n",
    "\n",
    "        dataset_pt1 = Subset(dataset, inds_pt1)\n",
    "        dataset_pt2 = Subset(dataset, inds_pt2)\n",
    "\n",
    "        return dataset_pt1, dataset_pt2\n",
    "    else:\n",
    "        raise ValueError('That mode is unknown')\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    dataset: Dataset,\n",
    "    random_state: int,\n",
    "    batch_size: int,\n",
    "    is_shuffle: bool,\n",
    "    num_workers: int,\n",
    "    is_pin_memory: bool\n",
    "):\n",
    "    \"\"\"Create a torch dataloader from a dataset\"\"\"\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(random_state)\n",
    "\n",
    "    torch_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=is_pin_memory,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "    return torch_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/home/storage/priceseekers/data/rosbank/rosbank_dataset/train_part/rosbank_targets.parquet'\n",
    "test_path = '/home/storage/priceseekers/data/rosbank/rosbank_dataset/test_part/rosbank_targets.parquet'\n",
    "path_to_save = '/home/storage/priceseekers/data/rosbank/rosbank_dataset/full/rosbank_targets.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pq.read_table(train_path).to_pandas()\n",
    "df_test = pq.read_table(test_path).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pa.Table.from_pandas(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(df_full, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".to_parquet(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = '/home/storage/priceseekers/data/rosbank/rosbank_dataset/train_part/rosbank_pca_embeddings.parquet'\n",
    "features_dim = 112\n",
    "targets_path = '/home/storage/priceseekers/data/rosbank/rosbank_dataset/train_part/rosbank_targets.parquet'\n",
    "path_to_file_names_to_be_excluded  = None\n",
    "\n",
    "random_state = 1\n",
    "split_fraction = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_pt1, inds_pt2 = split_array_into_twoparts_by_inds(\n",
    "    example_names, \n",
    "    random_state,\n",
    "    split_fraction\n",
    ")\n",
    "\n",
    "dataset_pt1 = Subset(dataset, inds_pt1)\n",
    "dataset_pt2 = Subset(dataset, inds_pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.9074e+00, -8.2724e-01, -5.9442e-01, -9.3966e-01,  1.0408e-01,\n",
       "          3.1565e-02, -2.6306e-01, -7.4423e-01,  4.8671e-01,  4.8730e-01,\n",
       "         -4.8939e-01, -5.9958e-02, -2.9994e-01, -1.1231e-01,  1.1989e-01,\n",
       "          7.5383e-02,  1.4384e-01,  1.7608e-01,  1.4159e-01, -4.1005e-02,\n",
       "         -3.2129e-02, -2.3837e-01,  7.4586e-02, -1.1077e-02, -4.4204e-02,\n",
       "         -1.2154e-02, -1.6696e-01,  1.4630e-02,  1.2890e-01, -1.3122e-01,\n",
       "          2.1491e-02, -3.4994e-02, -3.0660e-02, -2.3411e-02, -2.8227e-02,\n",
       "         -1.8308e-02,  9.7461e-04,  6.2755e-03, -2.6535e-02,  1.5229e-02,\n",
       "          1.6535e-02, -6.9116e-03, -1.1298e-02,  1.2000e-02, -6.5124e-03,\n",
       "         -3.6738e-03, -3.4251e-03, -1.6331e-04,  2.3690e-03,  1.4011e-02,\n",
       "          2.2371e-02,  3.0301e-03, -1.3432e-02,  2.1075e-02,  9.9930e-03,\n",
       "          1.0596e-02, -1.7536e-02, -1.0270e-03, -1.3394e-02, -1.6774e-03,\n",
       "          2.2909e-03,  5.5026e-03, -6.6627e-03, -5.0395e-04, -5.1123e-03,\n",
       "         -4.4063e-03,  1.0375e-03, -4.1087e-03,  1.2138e-02,  5.1595e-03,\n",
       "         -1.4110e-03, -7.7432e-03, -5.1045e-03, -3.4200e-04, -3.2404e-03,\n",
       "         -5.5575e-14, -8.0171e-14,  4.1647e-14,  9.5612e-15,  6.2139e-14,\n",
       "          3.2446e-15,  1.8145e-14, -5.5078e-14,  1.2415e-14, -1.8735e-14,\n",
       "         -6.8479e-15,  2.0752e-14, -2.0297e-14, -6.2173e-14, -7.3033e-15,\n",
       "         -7.1167e-15,  1.7871e-14,  2.5116e-14,  4.3264e-14,  3.0637e-02,\n",
       "         -3.7394e-02,  1.2274e-01, -6.8220e-02, -1.0600e-01,  1.1215e-01,\n",
       "         -1.3541e-01,  1.6370e-01, -1.1982e-01, -1.4429e-01, -1.4719e-01,\n",
       "          9.1255e-02, -1.2792e-01, -6.7232e-03,  1.4761e-01, -6.8350e-02,\n",
       "         -5.6674e-01,  4.2268e-01]),\n",
       " tensor(1, dtype=torch.int32),\n",
       " 3850)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pt2[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
